{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90b20c91",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37e86314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation already completed, skipping...\n",
      "Current working directory: c:\\Users\\Eu\\Desktop\\PROJETO_PESS_DADOS\\src\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Define paths based on environment\n",
    "if Path(\"/kaggle\").exists():\n",
    "    os.environ[\"AMBIENTE\"] = \"KAGGLE\"\n",
    "    os.environ[\"TENSORBOARD_NO_TF\"] = \"1\"\n",
    "\n",
    "    PATH_DATASET = Path(\"/kaggle/working/STROKE_PREDICTION\")\n",
    "    PATH_CODE = PATH_DATASET / \"src\"\n",
    "    PATH_OUTPUT_DIR = PATH_DATASET\n",
    "elif Path(\"/content\").exists():\n",
    "    os.environ[\"AMBIENTE\"] = \"COLAB\"\n",
    "    PATH_DATASET = Path(\"/content/DELETAR\")\n",
    "    PATH_CODE = PATH_DATASET / \"src\"\n",
    "    PATH_OUTPUT_DIR = PATH_DATASET / \"outputs\"\n",
    "else:\n",
    "    os.environ[\"AMBIENTE\"] = \"LOCAL\"\n",
    "    PATH_CODE = Path.cwd()\n",
    "    PATH_DATASET = PATH_CODE.parent\n",
    "    PATH_OUTPUT_DIR = PATH_DATASET / \"outputs\"\n",
    "\n",
    "\n",
    "# Check if installation has been done\n",
    "INSTALL_MARKER = PATH_DATASET / \".install_complete\"\n",
    "try:\n",
    "    if not INSTALL_MARKER.exists():\n",
    "        # Install uv\n",
    "        pass\n",
    "        !pip install uv\n",
    "\n",
    "        # Environment-specific setup\n",
    "        if os.environ[\"AMBIENTE\"] == \"KAGGLE\":\n",
    "            import kaggle_secrets\n",
    "\n",
    "            user_secrets = kaggle_secrets.UserSecretsClient()\n",
    "            github_pat = user_secrets.get_secret(\"GITHUB_PAT\")\n",
    "\n",
    "            os.chdir(\"/kaggle/working\")\n",
    "            os.system(\n",
    "                f\"git clone https://{github_pat}@github.com/lfaoliveira/STROKE_PREDICTION.git\"\n",
    "            )\n",
    "            os.chdir(PATH_DATASET)\n",
    "\n",
    "        elif os.environ[\"AMBIENTE\"] == \"LOCAL\":\n",
    "            os.system(\"git pull origin main\")\n",
    "\n",
    "        # Install dependencies\n",
    "        os.chdir(PATH_DATASET)\n",
    "        os.system(\"uv pip install --requirements pyproject.toml --system\")\n",
    "\n",
    "        if os.environ[\"AMBIENTE\"] == \"KAGGLE\":\n",
    "            os.system(\n",
    "                \"uv pip install --upgrade --force-reinstall --no-cache-dir scipy numpy matplotlib protobuf tensorboard\"\n",
    "            )\n",
    "\n",
    "        # Mark installation as complete\n",
    "        INSTALL_MARKER.touch()\n",
    "        print(\"Installation completed\")\n",
    "    else:\n",
    "        print(\"Installation already completed, skipping...\")\n",
    "\n",
    "    os.chdir(PATH_CODE)\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "except Exception:\n",
    "    print(\"FALHA AO INICIAR NOTEBOOK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f226e767",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ef0119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataProcesser.dataset import StrokeDataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================\n",
    "# Load data\n",
    "# =========================\n",
    "df = StrokeDataset().original_df.copy()\n",
    "\n",
    "# =========================\n",
    "# 1. Dataset overview\n",
    "# =========================\n",
    "print(\"Shape:\", df.shape)\n",
    "print(df.info())\n",
    "print(\"\\nMissing values:\\n\", df.isna().sum())\n",
    "\n",
    "# =========================\n",
    "# 2. Target distribution\n",
    "# =========================\n",
    "stroke_dist = df[\"stroke\"].value_counts(normalize=True) * 100\n",
    "print(\"\\nStroke distribution (%):\\n\", stroke_dist)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# =========================\n",
    "# 1. Stroke distribution\n",
    "# =========================\n",
    "df[\"stroke\"].value_counts().sort_index().plot(kind=\"bar\", ax=axes[0])\n",
    "axes[0].set_title(\"Stroke Distribution (Imbalanced)\")\n",
    "axes[0].set_xticks([0, 1])\n",
    "axes[0].set_xticklabels([\"No Stroke\", \"Stroke\"], rotation=0)\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].set_yticks([i for i in range(0, 4501, 500)])\n",
    "# =========================\n",
    "# 2. Age vs Stroke\n",
    "# =========================\n",
    "df.boxplot(column=\"age\", by=\"stroke\", ax=axes[1])\n",
    "axes[1].set_title(\"Age by Stroke\")\n",
    "axes[1].set_xlabel(\"Stroke\")\n",
    "axes[1].set_ylabel(\"Age\")\n",
    "\n",
    "# =========================\n",
    "# 3. Glucose vs Stroke\n",
    "# =========================\n",
    "df.boxplot(column=\"avg_glucose_level\", by=\"stroke\", ax=axes[2])\n",
    "axes[2].set_title(\"Avg Glucose by Stroke\")\n",
    "axes[2].set_xlabel(\"Stroke\")\n",
    "axes[2].set_ylabel(\"Glucose Level\")\n",
    "\n",
    "# =========================\n",
    "# 4. BMI vs Stroke\n",
    "# =========================\n",
    "df.boxplot(column=\"bmi\", by=\"stroke\", ax=axes[3])\n",
    "axes[3].set_title(\"BMI by Stroke\")\n",
    "axes[3].set_xlabel(\"Stroke\")\n",
    "axes[3].set_ylabel(\"BMI\")\n",
    "\n",
    "# =========================\n",
    "# 5. Hypertension\n",
    "# =========================\n",
    "pd.crosstab(df[\"hypertension\"], df[\"stroke\"], normalize=\"index\").mul(100).plot(\n",
    "    kind=\"bar\", ax=axes[4], rot=0\n",
    ")\n",
    "axes[4].set_title(\"Stroke Rate by Hypertension\")\n",
    "axes[4].set_ylabel(\"Percentage (%)\")\n",
    "axes[4].legend([\"No Stroke\", \"Stroke\"])\n",
    "\n",
    "# =========================\n",
    "# 6. Heart disease\n",
    "# =========================\n",
    "pd.crosstab(df[\"heart_disease\"], df[\"stroke\"], normalize=\"index\").mul(100).plot(\n",
    "    kind=\"bar\", ax=axes[5], rot=0\n",
    ")\n",
    "axes[5].set_title(\"Stroke Rate by Heart Disease\")\n",
    "axes[5].set_ylabel(\"Percentage (%)\")\n",
    "axes[5].legend([\"No Stroke\", \"Stroke\"])\n",
    "\n",
    "# =========================\n",
    "# 7. Smoking status\n",
    "# =========================\n",
    "pd.crosstab(df[\"smoking_status\"], df[\"stroke\"], normalize=\"index\").mul(100).sort_values(\n",
    "    by=1\n",
    ").plot(kind=\"barh\", ax=axes[6])\n",
    "axes[6].set_title(\"Stroke Rate by Smoking Status\")\n",
    "axes[6].set_xlabel(\"Percentage (%)\")\n",
    "axes[6].legend([\"No Stroke\", \"Stroke\"])\n",
    "\n",
    "# =========================\n",
    "# 8. Work type\n",
    "# =========================\n",
    "pd.crosstab(df[\"work_type\"], df[\"stroke\"], normalize=\"index\").mul(100).sort_values(\n",
    "    by=1\n",
    ").plot(kind=\"bar\", ax=axes[7], rot=30)\n",
    "axes[7].set_title(\"Stroke Rate by Work Type\")\n",
    "axes[7].set_ylabel(\"Percentage (%)\")\n",
    "axes[7].legend([\"No Stroke\", \"Stroke\"])\n",
    "\n",
    "# =========================\n",
    "# 9. Empty / summary slot\n",
    "# =========================\n",
    "axes[8].axis(\"off\")\n",
    "axes[8].set_title(\"EDA Summary Slot\")\n",
    "\n",
    "plt.suptitle(\"Stroke Dataset – Exploratory Data Analysis\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef61c38",
   "metadata": {},
   "source": [
    "## Normal training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222276af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Eu\\Desktop\\PROJETO_PESS_DADOS\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Seed set to 42\n",
      "2026/02/15 20:04:01 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2026/02/15 20:04:01 INFO mlflow.store.db.utils: Updating database tables\n",
      "2026/02/15 20:04:01 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/02/15 20:04:01 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2026/02/15 20:04:01 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/02/15 20:04:01 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "DATASET:\n",
      "          age  hypertension  heart_disease  avg_glucose_level       bmi  \\\n",
      "id                                                                        \n",
      "77  -1.324241     -0.318067      -0.228208          -0.438884 -1.310695   \n",
      "84   0.538054     -0.318067      -0.228208          -0.363242  0.331934   \n",
      "91  -0.038371     -0.318067      -0.228208          -0.152525 -1.323428   \n",
      "99  -0.526115     -0.318067      -0.228208           0.080704  2.980513   \n",
      "129 -0.836497     -0.318067      -0.228208          -0.174588 -0.342945   \n",
      "\n",
      "     gender_code  ever_married_code  work_type_code  Residence_type_code  \\\n",
      "id                                                                         \n",
      "77     -0.833023          -1.370831        1.674998            -1.014569   \n",
      "84      1.198428           0.729484       -0.155697             0.985640   \n",
      "91     -0.833023          -1.370831       -0.155697             0.985640   \n",
      "99     -0.833023          -1.370831       -0.155697             0.985640   \n",
      "129    -0.833023          -1.370831       -0.155697             0.985640   \n",
      "\n",
      "     smoking_status_code  \n",
      "id                        \n",
      "77             -1.288808  \n",
      "84              0.585232  \n",
      "91              0.585232  \n",
      "99             -1.288808  \n",
      "129             0.585232  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name         | Type             | Params | Mode  | In sizes | Out sizes\n",
      "---------------------------------------------------------------------------------\n",
      "0 | model        | Sequential       | 6.3 M  | train | [10]     | [2]      \n",
      "1 | val_metrics  | MetricCollection | 0      | train | ?        | ?        \n",
      "2 | test_metrics | MetricCollection | 0      | train | ?        | ?        \n",
      "---------------------------------------------------------------------------------\n",
      "6.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.3 M     Total params\n",
      "25.242    Total estimated model params size (MB)\n",
      "62        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "DATASET:\n",
      "          age  hypertension  heart_disease  avg_glucose_level       bmi  \\\n",
      "id                                                                        \n",
      "77  -1.324241     -0.318067      -0.228208          -0.438884 -1.310695   \n",
      "84   0.538054     -0.318067      -0.228208          -0.363242  0.331934   \n",
      "91  -0.038371     -0.318067      -0.228208          -0.152525 -1.323428   \n",
      "99  -0.526115     -0.318067      -0.228208           0.080704  2.980513   \n",
      "129 -0.836497     -0.318067      -0.228208          -0.174588 -0.342945   \n",
      "\n",
      "     gender_code  ever_married_code  work_type_code  Residence_type_code  \\\n",
      "id                                                                         \n",
      "77     -0.833023          -1.370831        1.674998            -1.014569   \n",
      "84      1.198428           0.729484       -0.155697             0.985640   \n",
      "91     -0.833023          -1.370831       -0.155697             0.985640   \n",
      "99     -0.833023          -1.370831       -0.155697             0.985640   \n",
      "129    -0.833023          -1.370831       -0.155697             0.985640   \n",
      "\n",
      "     smoking_status_code  \n",
      "id                        \n",
      "77             -1.288808  \n",
      "84              0.585232  \n",
      "91              0.585232  \n",
      "99             -1.288808  \n",
      "129             0.585232  \n",
      "\n",
      "Epoch 1: 100%|██████████| 123/123 [00:07<00:00, 16.05it/s, v_num=0577, train_loss_step=0.175, val_loss=0.264, train_loss_epoch=0.165] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 123/123 [00:08<00:00, 14.75it/s, v_num=0577, train_loss_step=0.175, val_loss=0.264, train_loss_epoch=0.165]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/15 20:05:43 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cpu) contains a local version label (+cpu). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2026/02/15 20:06:02 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.21.0+cpu) contains a local version label (+cpu). MLflow logged a pip requirement for this package as 'torchvision==0.21.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2026/02/15 20:06:03 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.1527, -1.5117],\n",
      "        [ 2.1527, -1.5117],\n",
      "        [ 2.1527, -1.5117],\n",
      "        ...,\n",
      "        [ 2.1527, -1.5117],\n",
      "        [ 2.1527, -1.5117],\n",
      "        [ 2.1527, -1.5117]]) \n",
      "\n",
      " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])\n",
      "CWD: c:\\Users\\Eu\\Desktop\\PROJETO_PESS_DADOS\\src\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH ZIPFILE: C:\\Users\\Eu\\Desktop\\PROJETO_PESS_DADOS\\src\\resultado_kaggle_stroke_MLP_1.zip\n",
      "\n",
      " ============================================================\n",
      "RESULTADOS ZIPADOS C:\\Users\\Eu\\Desktop\\PROJETO_PESS_DADOS\\src\\resultado_kaggle_stroke_MLP_1\n",
      "============================================================ \n",
      "\n",
      "\n",
      "\n",
      "DATASET:\n",
      "          age  hypertension  heart_disease  avg_glucose_level       bmi  \\\n",
      "id                                                                        \n",
      "77  -1.324241     -0.318067      -0.228208          -0.438884 -1.310695   \n",
      "84   0.538054     -0.318067      -0.228208          -0.363242  0.331934   \n",
      "91  -0.038371     -0.318067      -0.228208          -0.152525 -1.323428   \n",
      "99  -0.526115     -0.318067      -0.228208           0.080704  2.980513   \n",
      "129 -0.836497     -0.318067      -0.228208          -0.174588 -0.342945   \n",
      "\n",
      "     gender_code  ever_married_code  work_type_code  Residence_type_code  \\\n",
      "id                                                                         \n",
      "77     -0.833023          -1.370831        1.674998            -1.014569   \n",
      "84      1.198428           0.729484       -0.155697             0.985640   \n",
      "91     -0.833023          -1.370831       -0.155697             0.985640   \n",
      "99     -0.833023          -1.370831       -0.155697             0.985640   \n",
      "129    -0.833023          -1.370831       -0.155697             0.985640   \n",
      "\n",
      "     smoking_status_code  \n",
      "id                        \n",
      "77             -1.288808  \n",
      "84              0.585232  \n",
      "91              0.585232  \n",
      "99             -1.288808  \n",
      "129             0.585232  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Eu\\Desktop\\PROJETO_PESS_DADOS\\.venv\\Lib\\site-packages\\kan\\MultKAN.py:813: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1831.)\n",
      "  self.subnode_actscale.append(torch.std(x, dim=0).detach())\n",
      "c:\\Users\\Eu\\Desktop\\PROJETO_PESS_DADOS\\.venv\\Lib\\site-packages\\kan\\MultKAN.py:823: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1831.)\n",
      "  input_range = torch.std(preacts, dim=0) + 0.1\n",
      "c:\\Users\\Eu\\Desktop\\PROJETO_PESS_DADOS\\.venv\\Lib\\site-packages\\kan\\MultKAN.py:824: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1831.)\n",
      "  output_range_spline = torch.std(postacts_numerical, dim=0) # for training, only penalize the spline part\n",
      "c:\\Users\\Eu\\Desktop\\PROJETO_PESS_DADOS\\.venv\\Lib\\site-packages\\kan\\MultKAN.py:825: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1831.)\n",
      "  output_range = torch.std(postacts, dim=0) # for visualization, include the contribution from both spline + symbolic\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "c:\\Users\\Eu\\Desktop\\PROJETO_PESS_DADOS\\.venv\\Lib\\site-packages\\kan\\MultKAN.py:813: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1831.)\n",
      "  self.subnode_actscale.append(torch.std(x, dim=0).detach())\n",
      "c:\\Users\\Eu\\Desktop\\PROJETO_PESS_DADOS\\.venv\\Lib\\site-packages\\kan\\MultKAN.py:823: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1831.)\n",
      "  input_range = torch.std(preacts, dim=0) + 0.1\n",
      "c:\\Users\\Eu\\Desktop\\PROJETO_PESS_DADOS\\.venv\\Lib\\site-packages\\kan\\MultKAN.py:824: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1831.)\n",
      "  output_range_spline = torch.std(postacts_numerical, dim=0) # for training, only penalize the spline part\n",
      "c:\\Users\\Eu\\Desktop\\PROJETO_PESS_DADOS\\.venv\\Lib\\site-packages\\kan\\MultKAN.py:825: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1831.)\n",
      "  output_range = torch.std(postacts, dim=0) # for visualization, include the contribution from both spline + symbolic\n",
      "\n",
      "  | Name         | Type             | Params | Mode  | In sizes | Out sizes\n",
      "---------------------------------------------------------------------------------\n",
      "0 | model        | MultKAN          | 7.4 K  | train | [1, 10]  | [1, 2]   \n",
      "1 | val_metrics  | MetricCollection | 0      | train | ?        | ?        \n",
      "2 | test_metrics | MetricCollection | 0      | train | ?        | ?        \n",
      "---------------------------------------------------------------------------------\n",
      "6.0 K     Trainable params\n",
      "1.3 K     Non-trainable params\n",
      "7.4 K     Total params\n",
      "0.029     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "DATASET:\n",
      "          age  hypertension  heart_disease  avg_glucose_level       bmi  \\\n",
      "id                                                                        \n",
      "77  -1.324241     -0.318067      -0.228208          -0.438884 -1.310695   \n",
      "84   0.538054     -0.318067      -0.228208          -0.363242  0.331934   \n",
      "91  -0.038371     -0.318067      -0.228208          -0.152525 -1.323428   \n",
      "99  -0.526115     -0.318067      -0.228208           0.080704  2.980513   \n",
      "129 -0.836497     -0.318067      -0.228208          -0.174588 -0.342945   \n",
      "\n",
      "     gender_code  ever_married_code  work_type_code  Residence_type_code  \\\n",
      "id                                                                         \n",
      "77     -0.833023          -1.370831        1.674998            -1.014569   \n",
      "84      1.198428           0.729484       -0.155697             0.985640   \n",
      "91     -0.833023          -1.370831       -0.155697             0.985640   \n",
      "99     -0.833023          -1.370831       -0.155697             0.985640   \n",
      "129    -0.833023          -1.370831       -0.155697             0.985640   \n",
      "\n",
      "     smoking_status_code  \n",
      "id                        \n",
      "77             -1.288808  \n",
      "84              0.585232  \n",
      "91              0.585232  \n",
      "99             -1.288808  \n",
      "129             0.585232  \n",
      "\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from typing import Literal\n",
    "\n",
    "import gc\n",
    "import mlflow\n",
    "import torch\n",
    "from Models.mlp import MLP, MLPSearchSpace\n",
    "from Models.kan import MyKan, KANSearchSpace\n",
    "from lightning import seed_everything, Trainer\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "from mlflow.pytorch import autolog\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from DataProcesser.datamodule import StrokeDataModule\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def zip_res(\n",
    "    path_sqlite: str, path_mlflow: Path, filename: str, dest_folder: Path | None = None\n",
    "):\n",
    "    import shutil\n",
    "\n",
    "    path_sqlite_clean = path_sqlite.replace(\"sqlite:///\", \"\")\n",
    "    print(f\"CWD: {Path.cwd()}\\n\")\n",
    "    PATH_TEMP = Path.cwd() / \"ZIP_TEMP\"\n",
    "    shutil.rmtree(PATH_TEMP, ignore_errors=True)\n",
    "    PATH_TEMP.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    shutil.copy(path_sqlite_clean, PATH_TEMP / Path(path_sqlite_clean).name)\n",
    "    shutil.copytree(path_mlflow, PATH_TEMP / path_mlflow.name)\n",
    "\n",
    "    # Determine destination folder\n",
    "    if dest_folder is None:\n",
    "        dest_folder = Path.cwd()\n",
    "    else:\n",
    "        dest_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create zip file in destination folder\n",
    "    zip_path = dest_folder / filename.replace(\".zip\", \"\")\n",
    "    shutil.make_archive(str(zip_path), \"zip\", PATH_TEMP)\n",
    "    shutil.rmtree(PATH_TEMP)\n",
    "    print(f\"PATH ZIPFILE: {zip_path.with_suffix('.zip').resolve()}\")\n",
    "\n",
    "\n",
    "def model_choice(CHOICE, INPUT_DIMS, N_CLASSES):\n",
    "    recall_factor = 4.0\n",
    "    if CHOICE == \"MLP\":\n",
    "        search_space = MLPSearchSpace()\n",
    "\n",
    "        keys = search_space.Keys\n",
    "        hyperparams = {\n",
    "            keys.BATCH_SIZE: 32,\n",
    "            keys.HIDDEN_DIMS: 512,\n",
    "            keys.LR: 1e-3,\n",
    "            keys.WEIGHT_DECAY: 1e-5,\n",
    "            keys.BETA0: 0.900,\n",
    "            keys.BETA1: 0.99,\n",
    "            keys.N_LAYERS: 24,\n",
    "        }\n",
    "        suggested_hparams = search_space.suggest(hyperparams)\n",
    "        model = MLP(\n",
    "            INPUT_DIMS,\n",
    "            N_CLASSES,\n",
    "            recall_factor=recall_factor,\n",
    "            hyperparameters=suggested_hparams,\n",
    "        )\n",
    "    elif CHOICE == \"KAN\":\n",
    "        search_space = KANSearchSpace()\n",
    "        keys = search_space.Keys\n",
    "        hyperparams = {\n",
    "            keys.BATCH_SIZE: 32,\n",
    "            keys.HIDDEN_DIMS: 16,\n",
    "            keys.LR: 1e-3,\n",
    "            keys.WEIGHT_DECAY: 1e-5,\n",
    "            keys.BETA0: 0.900,\n",
    "            keys.BETA1: 0.99,\n",
    "            keys.GRID: 24,\n",
    "            keys.SPLINE_POL_ORDER: 3,\n",
    "        }\n",
    "        suggested_hparams = search_space.suggest(hyperparams)\n",
    "        model = MyKan(\n",
    "            INPUT_DIMS,\n",
    "            N_CLASSES,\n",
    "            recall_factor=recall_factor,\n",
    "            hyperparameters=suggested_hparams,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"ESCOLHA DE MODELO ERRADA!\")\n",
    "    return model, suggested_hparams, keys\n",
    "\n",
    "\n",
    "## -----------------------------COLAR NO KAGGLE------------------\n",
    "def main(CHOICE: str):\n",
    "    ###------SEEDS---------###\n",
    "    RAND_SEED = 42\n",
    "    seed_everything(RAND_SEED)\n",
    "    AMBIENTE = os.environ[\"AMBIENTE\"]\n",
    "    GPU = True if AMBIENTE in [\"KAGGLE\", \"COLAB\"] else False\n",
    "    ## ----------VARIAVEIS TREINO-----------\n",
    "    cpus = os.cpu_count()\n",
    "    WORKERS = cpus if cpus is not None else 1\n",
    "    NUM_DEVICES = 1 if GPU else 1\n",
    "    NUM_NODES = 1\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 2\n",
    "    PATIENCE = 20\n",
    "    ARTIFACT_PATH = PATH_OUTPUT_DIR / \"artifacts\"\n",
    "    os.makedirs(ARTIFACT_PATH, exist_ok=True)\n",
    "\n",
    "    #### -------- VARIAVEIS DE LOGGING ------------\n",
    "    EXP_NAME = f\"stroke_{CHOICE}_1\"\n",
    "    RUN_NAME: str | None = f\"normal_{CHOICE}\"\n",
    "    MLF_TRACK_URI = f\"sqlite:///{PATH_CODE}/mlflow.db\"\n",
    "\n",
    "    mlflow.set_tracking_uri(MLF_TRACK_URI)\n",
    "    mlflow.set_experiment(EXP_NAME)\n",
    "    autolog(log_models=True, checkpoint=True, exclusive=False)\n",
    "\n",
    "    ## ----------VARIAVEIS MODELO-----------\n",
    "    N_CLASSES = 2\n",
    "\n",
    "    datamodule = StrokeDataModule(BATCH_SIZE, WORKERS)\n",
    "    datamodule.prepare_data()\n",
    "    datamodule.setup(\"fit\")\n",
    "\n",
    "    INPUT_DIMS = datamodule.input_dims or -1\n",
    "    assert INPUT_DIMS > 0\n",
    "    model, hparams, keys = model_choice(CHOICE, INPUT_DIMS, N_CLASSES)\n",
    "\n",
    "    _ = model(model.example_input_array)\n",
    "\n",
    "    # loop principal de treinamento\n",
    "    with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "        active_run_id = run.info.run_id\n",
    "\n",
    "        mlflow_logger = MLFlowLogger(\n",
    "            experiment_name=EXP_NAME,\n",
    "            tracking_uri=MLF_TRACK_URI,\n",
    "            log_model=True,\n",
    "            run_id=active_run_id,\n",
    "        )\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=PATIENCE, mode=\"min\"\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            max_epochs=EPOCHS,\n",
    "            devices=NUM_DEVICES,\n",
    "            accelerator=\"gpu\" if GPU else \"cpu\",\n",
    "            num_nodes=NUM_NODES,\n",
    "            logger=mlflow_logger,\n",
    "            enable_checkpointing=False,\n",
    "            callbacks=[early_stopping],\n",
    "        )\n",
    "        trainer.fit(model, datamodule=datamodule)\n",
    "        mlflow.log_params(dict(model.hparams))\n",
    "\n",
    "        # Test and log artifacts (useful for the Analysis section)\n",
    "        test_df = datamodule.dataset.original_df.copy()\n",
    "        test_df[\"pred\"] = None\n",
    "        test_df[\"error\"] = None\n",
    "\n",
    "        # Perform one-pass test logic on the full test dataset\n",
    "        _, test_dataset = datamodule.test_dataloader()\n",
    "        return_dict = model.test_step(\n",
    "            test_dataset=test_dataset,\n",
    "            output_df=test_df,\n",
    "        )\n",
    "        test_df = return_dict[\"output_df\"]\n",
    "\n",
    "        # Log test metrics to MLFlow\n",
    "        test_results = model.test_metrics.compute()\n",
    "        mlflow.log_metrics(\n",
    "            {f\"test_{k}\": float(v) for k, v in test_results.items() if v.numel() == 1}\n",
    "        )\n",
    "        model.test_metrics.reset()\n",
    "\n",
    "        name = f\"test_results_{run.info.run_id}.csv\"\n",
    "        path_test_csv = Path(ARTIFACT_PATH, name)\n",
    "        test_df.to_csv(path_test_csv)\n",
    "        mlflow.log_artifact(str(path_test_csv))\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    NAME_RESZIP = f\"resultado_kaggle_{EXP_NAME}\"\n",
    "    MLRUNS_FOLDER = Path.cwd() / \"mlruns\"\n",
    "    zip_res(MLF_TRACK_URI, MLRUNS_FOLDER, NAME_RESZIP)\n",
    "    print(\"\\n\", \"=\" * 60)\n",
    "    print(f\"RESULTADOS ZIPADOS {Path(NAME_RESZIP).resolve()}\")\n",
    "    print(\"=\" * 60, \"\\n\")\n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        ARQ_TYPE = Literal[\"MLP\", \"KAN\", \"SVM\", \"XGBOOST\"]  ## MODEL ARCHITECTURE\n",
    "        models: list[ARQ_TYPE] = [\"MLP\", \"KAN\"]\n",
    "        for choice in models:\n",
    "            # trains model based on architecture\n",
    "            main(choice)\n",
    "\n",
    "        NAME_RESZIP = \"resultado_kaggle_stroke_normal\"\n",
    "        MLRUNS_FOLDER = Path.cwd() / \"mlruns\"\n",
    "        MLF_TRACK_URI = f\"sqlite:///{PATH_CODE}/mlflow.db\"\n",
    "        ZIP_ROOT = (\n",
    "            PATH_DATASET / \"..\" if os.environ[\"AMBIENTE\"] == \"KAGGLE\" else PATH_DATASET\n",
    "        )\n",
    "        zip_res(MLF_TRACK_URI, MLRUNS_FOLDER, NAME_RESZIP, ZIP_ROOT)\n",
    "        print(\"\\n\", \"=\" * 60)\n",
    "        print(f\"RESULTADOS ZIPADOS {Path(ZIP_ROOT, NAME_RESZIP).resolve()}\")\n",
    "        print(\"=\" * 60, \"\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    gc.collect()\n",
    "\n",
    "    if os.environ[\"AMBIENTE\"] == \"LOCAL\":\n",
    "        from view.dashboard import see_model\n",
    "\n",
    "        see_model(PATH_DATASET / \"mlflow.db\", PATH_DATASET / \"..\" / \"mlruns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb567387",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6528cdda",
   "metadata": {},
   "source": [
    "## Training with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efa0017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from typing import Literal\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from Models.mlp import MLP, MLPSearchSpace\n",
    "from lightning import Callback, seed_everything, Trainer\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "from mlflow.pytorch import autolog\n",
    "from DataProcesser.datamodule import StrokeDataModule\n",
    "import optuna\n",
    "from Models.kan import KANSearchSpace, MyKan\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def zip_res(\n",
    "    path_sqlite: str, path_mlflow: Path, filename: str, dest_folder: Path | None = None\n",
    "):\n",
    "    import shutil\n",
    "\n",
    "    path_sqlite_clean = path_sqlite.replace(\"sqlite:///\", \"\")\n",
    "    print(f\"CWD: {Path.cwd()}\\n\")\n",
    "    PATH_TEMP = Path.cwd() / \"ZIP_TEMP\"\n",
    "    shutil.rmtree(PATH_TEMP, ignore_errors=True)\n",
    "    PATH_TEMP.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    shutil.copy(path_sqlite_clean, PATH_TEMP / Path(path_sqlite_clean).name)\n",
    "    shutil.copytree(path_mlflow, PATH_TEMP / path_mlflow.name)\n",
    "\n",
    "    # Determine destination folder\n",
    "    if dest_folder is None:\n",
    "        dest_folder = Path.cwd()\n",
    "    else:\n",
    "        dest_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create zip file in destination folder\n",
    "    zip_path = dest_folder / filename.replace(\".zip\", \"\")\n",
    "    shutil.make_archive(str(zip_path), \"zip\", PATH_TEMP)\n",
    "    shutil.rmtree(PATH_TEMP)\n",
    "    print(f\"PATH ZIPFILE: {zip_path.with_suffix('.zip').resolve()}\")\n",
    "\n",
    "\n",
    "def supress_warnings():\n",
    "    import logging\n",
    "\n",
    "    # Suppress specific MLflow warnings\n",
    "    logging.getLogger(\"mlflow.utils.requirements_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "    # Suppress PyTorch Lightning info messages\n",
    "    logging.getLogger(\"pytorch_lightning.utilities.rank_zero\").setLevel(logging.ERROR)\n",
    "\n",
    "    # Suppress Optuna info messages\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "\n",
    "def model_choice(\n",
    "    CHOICE, INPUT_DIMS, trial: optuna.trial.Trial, N_CLASSES, hyperparameters=None\n",
    "):\n",
    "    if CHOICE == \"MLP\":\n",
    "        search_space = MLPSearchSpace()\n",
    "        suggested_hparams = search_space.suggest_optuna(trial)\n",
    "        keys = search_space.Keys\n",
    "        model = MLP(INPUT_DIMS, N_CLASSES,recall_factor=1.8, hyperparameters=suggested_hparams)\n",
    "    elif CHOICE == \"KAN\":\n",
    "        search_space = KANSearchSpace()\n",
    "        suggested_hparams = search_space.suggest_optuna(trial)\n",
    "        keys = search_space.Keys\n",
    "        model = MyKan(INPUT_DIMS, N_CLASSES,recall_factor=1.8, hyperparameters=suggested_hparams)\n",
    "    else:\n",
    "        raise ValueError(\"ESCOLHA DE MODELO ERRADA!\")\n",
    "    return model, suggested_hparams, keys\n",
    "\n",
    "\n",
    "## -----------------------------COLAR NO KAGGLE------------------\n",
    "def main(CHOICE: str, MLF_TRACK_URI: str):\n",
    "    ###------SEEDS---------###\n",
    "    RAND_SEED = 42\n",
    "    seed_everything(RAND_SEED)\n",
    "    AMBIENTE = os.environ[\"AMBIENTE\"]\n",
    "    GPU = True if AMBIENTE in [\"KAGGLE\", \"COLAB\"] else False\n",
    "    ## ----------VARIAVEIS TREINO-----------\n",
    "    cpus = os.cpu_count()\n",
    "    WORKERS = cpus if cpus is not None else 1\n",
    "    NUM_DEVICES = 1 if GPU else 1\n",
    "    NUM_NODES = 1\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 2\n",
    "    TRIALS = 1\n",
    "    PATIENCE = 25\n",
    "    EARLY_STOP = True\n",
    "    PRINT_MODEL_SUMMARY = False\n",
    "    ARTIFACT_PATH = PATH_OUTPUT_DIR / \"artifacts\"\n",
    "    os.makedirs(ARTIFACT_PATH, exist_ok=True)\n",
    "\n",
    "    #### -------- VARIAVEIS DE LOGGING ------------\n",
    "    EXP_NAME = f\"stroke_{CHOICE}_1\"\n",
    "    RUN_NAME: str | None = None  # nome da RUN: pode ser aleatório ou definido\n",
    "\n",
    "    mlflow.set_tracking_uri(MLF_TRACK_URI)\n",
    "    mlflow.set_experiment(EXP_NAME)\n",
    "    autolog(log_models=True, checkpoint=True, exclusive=False)\n",
    "\n",
    "    ## ----------VARIAVEIS MODELO-----------\n",
    "    N_CLASSES = 2\n",
    "\n",
    "    datamodule = StrokeDataModule(BATCH_SIZE, WORKERS)\n",
    "\n",
    "    datamodule.prepare_data()\n",
    "    datamodule.setup(\"fit\")\n",
    "\n",
    "    INPUT_DIMS = datamodule.input_dims or -1\n",
    "    assert INPUT_DIMS > 0\n",
    "\n",
    "    # Progress bar for trials\n",
    "    pbar = tqdm(\n",
    "        total=TRIALS,\n",
    "        desc=f\"Optuna Trials ({CHOICE})\",\n",
    "        position=0,\n",
    "        leave=True,\n",
    "        colour=\"green\",\n",
    "    )\n",
    "\n",
    "    # loop principal de treinamento\n",
    "    def objective(trial: optuna.Trial):\n",
    "\n",
    "        model, hyperparameters, keys = model_choice(\n",
    "            CHOICE,\n",
    "            INPUT_DIMS,\n",
    "            trial,\n",
    "            N_CLASSES,\n",
    "        )\n",
    "\n",
    "        batch_size = hyperparameters[keys.BATCH_SIZE]\n",
    "\n",
    "        # Recreate dataloaders with trial batch_size\n",
    "        train_loader, val_loader = (\n",
    "            datamodule.train_dataloader(batch_size),\n",
    "            datamodule.val_dataloader(batch_size),\n",
    "        )\n",
    "        # model hyperparameters\n",
    "        hyperparameters = None\n",
    "\n",
    "        _ = model(model.example_input_array)\n",
    "\n",
    "        with mlflow.start_run(run_name=f\"trial_{trial.number}\", nested=True) as run:\n",
    "            active_run_id = run.info.run_id\n",
    "\n",
    "            mlflow_logger = MLFlowLogger(\n",
    "                experiment_name=EXP_NAME,\n",
    "                tracking_uri=MLF_TRACK_URI,\n",
    "                log_model=True,\n",
    "                run_id=active_run_id,\n",
    "            )\n",
    "\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor=\"val_loss\", patience=PATIENCE, mode=\"min\"\n",
    "            )\n",
    "            callbacks: list[Callback] | None = [early_stopping] if EARLY_STOP else None\n",
    "\n",
    "            trainer = Trainer(\n",
    "                max_epochs=EPOCHS,\n",
    "                devices=NUM_DEVICES,\n",
    "                accelerator=\"gpu\" if GPU else \"cpu\",\n",
    "                num_nodes=NUM_NODES,\n",
    "                logger=mlflow_logger,\n",
    "                enable_checkpointing=False,  # must be disabled for mlflow correct logging\n",
    "                enable_model_summary=PRINT_MODEL_SUMMARY,\n",
    "                enable_progress_bar=False,  # Disable PyTorch Lightning progress bar, to avoid log polution\n",
    "                callbacks=callbacks,\n",
    "            )\n",
    "\n",
    "            trainer.fit(\n",
    "                model, train_dataloaders=train_loader, val_dataloaders=val_loader\n",
    "            )\n",
    "            mlflow.log_params(dict(model.hparams))\n",
    "            # mlflow_log_model(model, artifact_path=\"model\")\n",
    "            val_loss = trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "            test_df = datamodule.dataset.original_df.copy()\n",
    "            test_df[\"pred\"] = None\n",
    "            test_df[\"error\"] = None\n",
    "\n",
    "            test_loader, test_dataset = datamodule.test_dataloader(batch_size)\n",
    "            for batch_idx, test_batch in enumerate(test_loader):\n",
    "                test_df = model.test_step(\n",
    "                    batch=test_batch,\n",
    "                    batch_idx=batch_idx,\n",
    "                    output_df=test_df,\n",
    "                    test_dataset=test_dataset,\n",
    "                )\n",
    "\n",
    "            name = f\"test_results_{run.info.run_id}.csv\"\n",
    "            path_test_csv = Path(ARTIFACT_PATH, name)\n",
    "            # Ensure no directory exists with the same name before saving the file\n",
    "            if path_test_csv.exists() and path_test_csv.is_dir():\n",
    "                import shutil\n",
    "                shutil.rmtree(path_test_csv)\n",
    "            \n",
    "            test_df.to_csv(path_test_csv)\n",
    "            assert test_df is not None and path_test_csv.exists()\n",
    "            mlflow.log_artifact(str(path_test_csv))\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({\"val_loss\": f\"{val_loss:.4f}\", \"trial\": trial.number})\n",
    "            return val_loss\n",
    "\n",
    "    with mlflow.start_run(run_name=RUN_NAME) as parent_run:\n",
    "        study = optuna.create_study(direction=\"minimize\", study_name=f\"{CHOICE}\")\n",
    "        study.optimize(objective, n_trials=TRIALS, gc_after_trial=True)\n",
    "\n",
    "        # Log best parameters\n",
    "        mlflow.log_params(\n",
    "            {\"best_\" + k: v for k, v in study.best_trial.params.items()},\n",
    "            run_id=parent_run.info.run_id,\n",
    "        )\n",
    "\n",
    "        mlflow.log_metric(\n",
    "            \"best_val_loss\",\n",
    "            study.best_trial.value or float(\"inf\"),\n",
    "            run_id=parent_run.info.run_id,\n",
    "        )\n",
    "\n",
    "        print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "        print(\"Best validation loss:\", study.best_trial.value)\n",
    "        best_run_id = study.best_trial.user_attrs.get(\"run_id\")\n",
    "\n",
    "        mlflow.log_params(\n",
    "            {\"best_trial_id\": best_run_id},\n",
    "            run_id=parent_run.info.run_id,\n",
    "        )\n",
    "\n",
    "        # Close progress bar\n",
    "        pbar.close()\n",
    "\n",
    "        # Identify and tag the best run (no rename_run in MLflow Python API)\n",
    "        experiment = mlflow.get_experiment_by_name(EXP_NAME)\n",
    "        if experiment:\n",
    "            runs_df = pd.DataFrame(\n",
    "                mlflow.search_runs(\n",
    "                    experiment_ids=[experiment.experiment_id],\n",
    "                    order_by=[\"metrics.val_loss ASC\"],\n",
    "                )\n",
    "            )\n",
    "            runs_df = runs_df.dropna(subset=[\"metrics.val_loss\"])\n",
    "            if not runs_df.empty:\n",
    "                best_run_id = runs_df.iloc[0].run_id\n",
    "                prefix = os.environ[\"OPTUNA_BEST_RUN_PREFIX\"]\n",
    "                mlflow.MlflowClient().set_tag(\n",
    "                    best_run_id, \"mlflow.runName\", f\"{prefix}_{CHOICE}\"\n",
    "                )\n",
    "            else:\n",
    "                raise ModuleNotFoundError(\"Runs Dataframe empty\\n\")\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        ARQ_TYPE = Literal[\n",
    "            \"MLP\", \"KAN\", \"SVM\", \"XGBOOST\", \"RNDFOREST\", \"LIQUIDNN\"\n",
    "        ]  ## MODEL ARCHITECTURE\n",
    "        models: list[ARQ_TYPE] = [\"MLP\", \"KAN\"]\n",
    "        MLF_TRACK_URI = f\"sqlite:///{PATH_CODE}/mlflow.db\"\n",
    "\n",
    "        os.environ[\"OPTUNA\"] = \"True\"\n",
    "        os.environ[\"OPTUNA_BEST_RUN_PREFIX\"] = \"best_run\"\n",
    "\n",
    "        supress_warnings()\n",
    "        for i, choice in enumerate(models):\n",
    "            # trains model based on architecture\n",
    "            clear_output(wait=True)\n",
    "            main(choice, MLF_TRACK_URI)\n",
    "\n",
    "        NAME_RESZIP = \"resultado_kaggle\"\n",
    "        MLRUNS_FOLDER = PATH_CODE / \"mlruns\"\n",
    "        ZIP_ROOT = (\n",
    "            PATH_DATASET / \"..\" if os.environ[\"AMBIENTE\"] == \"KAGGLE\" else PATH_DATASET\n",
    "        )\n",
    "        zip_res(MLF_TRACK_URI, MLRUNS_FOLDER, NAME_RESZIP, ZIP_ROOT)\n",
    "        print(\"\\n\", \"=\" * 60)\n",
    "        print(f\"RESULTADOS ZIPADOS {Path(ZIP_ROOT, NAME_RESZIP).resolve()}\")\n",
    "        print(\"=\" * 60, \"\\n\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"PREMATURELY INTERRUPTING...\\n\")\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    gc.collect()\n",
    "\n",
    "    if os.environ[\"AMBIENTE\"] == \"LOCAL\":\n",
    "        from view.dashboard import see_model\n",
    "\n",
    "        see_model(PATH_DATASET / \"mlflow.db\", PATH_DATASET / \"..\" / \"mlruns\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb81704",
   "metadata": {},
   "source": [
    "## Results Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "239c18bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Eu\\Desktop\\PROJETO_PESS_DADOS\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/15 19:53:07 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2026/02/15 19:53:07 INFO mlflow.store.db.utils: Updating database tables\n",
      "2026/02/15 19:53:07 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/02/15 19:53:07 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2026/02/15 19:53:07 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/02/15 19:53:07 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_loss_step', 'epoch', 'val_loss', 'f_beta', 'prec', 'rec', 'train_loss', 'train_loss_epoch', 'test_auroc', 'test_f_beta', 'test_prec', 'test_rec']\n",
      "['train_loss_step', 'epoch', 'val_loss', 'f_beta', 'prec', 'rec', 'train_loss', 'train_loss_epoch']\n",
      "['train_loss_step', 'epoch', 'val_loss', 'f_beta', 'prec', 'rec', 'train_loss', 'train_loss_epoch']\n",
      "['train_loss_step', 'epoch', 'val_loss', 'f_beta', 'prec', 'rec', 'train_loss', 'train_loss_epoch']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'metrics.val_f_beta_avg'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_54080\\2597404199.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     15\u001b[39m \n\u001b[32m     16\u001b[39m SORT_METRIC = \u001b[33m\"val_f_beta_avg\"\u001b[39m\n\u001b[32m     17\u001b[39m RESIDUAL = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     18\u001b[39m print()\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m compare_df, resProcesser = final_analysis(\n\u001b[32m     20\u001b[39m     models, output_dir, SORT_METRIC, residual=RESIDUAL\n\u001b[32m     21\u001b[39m )\n\u001b[32m     22\u001b[39m \n",
      "\u001b[32mc:\\Users\\Eu\\Desktop\\PROJETO_PESS_DADOS\\src\\DataProcesser\\utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(models, output_dir, sort_metric, residual)\u001b[39m\n\u001b[32m    128\u001b[39m \n\u001b[32m    129\u001b[39m         \u001b[38;5;66;03m# armazena modelos de erro e dataframe\u001b[39;00m\n\u001b[32m    130\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m residual \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m runs.empty:\n\u001b[32m    131\u001b[39m             ascending = \u001b[33m\"loss\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m sort_metric\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m             best_run = runs.sort_values(\n\u001b[32m    133\u001b[39m                 f\"metrics.{sort_metric}\", ascending=ascending\n\u001b[32m    134\u001b[39m             ).iloc[\u001b[32m0\u001b[39m]\n\u001b[32m    135\u001b[39m             residual_analysis(client, best_run, choice, processer, plot=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[32mc:\\Users\\Eu\\Desktop\\PROJETO_PESS_DADOS\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[39m\n\u001b[32m   7207\u001b[39m             )\n\u001b[32m   7208\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m len(by):\n\u001b[32m   7209\u001b[39m             \u001b[38;5;66;03m# len(by) == 1\u001b[39;00m\n\u001b[32m   7210\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m7211\u001b[39m             k = self._get_label_or_level_values(by[\u001b[32m0\u001b[39m], axis=axis)\n\u001b[32m   7212\u001b[39m \n\u001b[32m   7213\u001b[39m             \u001b[38;5;66;03m# need to rewrap column in Series to apply key function\u001b[39;00m\n\u001b[32m   7214\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32mc:\\Users\\Eu\\Desktop\\PROJETO_PESS_DADOS\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1910\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1911\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1912\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1913\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1915\u001b[39m \n\u001b[32m   1916\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1917\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'metrics.val_f_beta_avg'"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from typing import Literal\n",
    "from DataProcesser.utils import final_analysis\n",
    "\n",
    "# Saves directly to env output dir\n",
    "output_dir = PATH_OUTPUT_DIR\n",
    "if os.environ[\"AMBIENTE\"] == \"KAGGLE\":\n",
    "    output_dir = PATH_DATASET.parent\n",
    "\n",
    "# Set MLflow tracking URI\n",
    "mlflow.set_tracking_uri(f\"sqlite:///{PATH_CODE}/mlflow.db\")\n",
    "ARQ_TYPE = Literal[\"MLP\", \"KAN\", \"SVM\", \"XGBOOST\"]  ## MODEL ARCHITECTURE\n",
    "models: list[ARQ_TYPE] = [\"MLP\", \"KAN\"]\n",
    "os.environ[\"OPTUNA\"] = \"True\"\n",
    "\n",
    "SORT_METRIC = \"val_f_beta_avg\"\n",
    "RESIDUAL = True\n",
    "print()\n",
    "compare_df, resProcesser = final_analysis(\n",
    "    models, output_dir, SORT_METRIC, residual=RESIDUAL\n",
    ")\n",
    "\n",
    "compare_df = compare_df.drop(columns=\"epoch_avg\")\n",
    "compare_df.to_csv(output_dir / \"classify_results.csv\")\n",
    "print(compare_df.to_string())\n",
    "\n",
    "ser_fbeta = compare_df[SORT_METRIC].sort_values(ascending=False)\n",
    "best_model, fbeta_value = next(ser_fbeta.items())\n",
    "print(f\"BEST MODEL: *{best_model}* WITH F-BETA: {fbeta_value}\\n\")\n",
    "\n",
    "# If true, execute residual analysis of best model's errors\n",
    "if RESIDUAL and False:\n",
    "    resProcesser.fit_predict(str(best_model))\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81c39d6",
   "metadata": {},
   "source": [
    "## MLFlow's Dashboard (Only works outside of Kaggle)\n",
    "### Download the training results from Kaggle and paste them into a cloned folder of the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1cfda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "\n",
    "\n",
    "def see_model(database: pathlib.Path, folder: pathlib.Path):\n",
    "    subprocess.Popen(\n",
    "        [\n",
    "            \"mlflow\",\n",
    "            \"ui\",\n",
    "            \"--backend-store-uri\",\n",
    "            f\"sqlite:///{database}\",\n",
    "            \"--default-artifact-root\",\n",
    "            folder,\n",
    "            \"--host\",\n",
    "            \"127.0.0.1\",\n",
    "            \"--port\",\n",
    "            \"5000\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    PATH_RES_ZIPADO = Path(\n",
    "        \"C:\\\\Users\\\\LUIS_FELIPE\\\\Downloads\\\\resultado_kaggle_stroke_1.zip\"\n",
    "    )\n",
    "    DIR = Path(Path.cwd(), PATH_RES_ZIPADO.name.replace(\".zip\", \"\"))\n",
    "    print(f\"DIR: {DIR}\")\n",
    "    if DIR.exists():\n",
    "        shutil.rmtree(DIR)\n",
    "    DIR.mkdir()\n",
    "    shutil.unpack_archive(PATH_RES_ZIPADO, DIR)\n",
    "\n",
    "    print(\"COMECANDO SUBPROCESSO!\\n\")\n",
    "    see_model(DIR / \"mlflow.db\", DIR / \"mlruns\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projeto-kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
